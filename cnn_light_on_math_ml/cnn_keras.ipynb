{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Networks: Light on Math Machine Learning\n",
    "\n",
    "Here we implement a simple CNN that is trained and tested on MNIST dataset. We use Keras; a high level TensorFlow sublibrary to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thushan\\documents\\python_virtualenvs\\tensorflow_venv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Conv2D,Dense,MaxPool2D,BatchNormalization,Flatten\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "# Required for Data downaload and preparation\n",
    "import struct\n",
    "import gzip\n",
    "import os\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data and Preprocessing\n",
    "\n",
    "Here we download the data (MNIST) dataset and perform some reshaping to make data `[28,28,1]` shape and normalize data into [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified train-images-idx3-ubyte.gz\n",
      "Found and verified train-labels-idx1-ubyte.gz\n",
      "Found and verified t10k-images-idx3-ubyte.gz\n",
      "Found and verified t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "Reading files train-images-idx3-ubyte.gz and train-labels-idx1-ubyte.gz\n",
      "60000 28 28\n",
      "(Images) Returned a tensor of shape  (60000, 28, 28, 1)\n",
      "(Labels) Returned a tensor of shape: 60000\n",
      "Sample labels:  [5 0 4 1 9 2 1 3 1 4]\n",
      "\n",
      "Reading files t10k-images-idx3-ubyte.gz and t10k-labels-idx1-ubyte.gz\n",
      "10000 28 28\n",
      "(Images) Returned a tensor of shape  (10000, 28, 28, 1)\n",
      "(Labels) Returned a tensor of shape: 10000\n",
      "Sample labels:  [7 2 1 0 4 1 4 9 5 9]\n",
      "\n",
      "Train size:  60000\n",
      "\n",
      "Test size:  10000\n"
     ]
    }
   ],
   "source": [
    "def maybe_download(url, filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if force or not os.path.exists(filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "\n",
    "def read_mnist(fname_img, fname_lbl, one_hot=False):\n",
    "    print('\\nReading files %s and %s'%(fname_img, fname_lbl))\n",
    "    \n",
    "    # Processing images\n",
    "    with gzip.open(fname_img) as fimg:        \n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        print(num,rows,cols)\n",
    "        img = (np.frombuffer(fimg.read(num*rows*cols), dtype=np.uint8).reshape(num, rows, cols,1)).astype(np.float32)\n",
    "        print('(Images) Returned a tensor of shape ',img.shape)\n",
    "        \n",
    "        img *= 1.0 / 255.0\n",
    "    \n",
    "    # Processing labels\n",
    "    with gzip.open(fname_lbl) as flbl:\n",
    "        # flbl.read(8) reads upto 8 bytes\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))               \n",
    "        lbl = np.frombuffer(flbl.read(num), dtype=np.int8)\n",
    "        if one_hot:\n",
    "            one_hot_lbl = np.zeros(shape=(num,10),dtype=np.float32)\n",
    "            one_hot_lbl[np.arange(num),lbl] = 1.0\n",
    "        print('(Labels) Returned a tensor of shape: %s'%lbl.shape)\n",
    "        print('Sample labels: ',lbl[:10])\n",
    "    \n",
    "    if not one_hot:\n",
    "        return img, lbl\n",
    "    else:\n",
    "        return img, one_hot_lbl\n",
    "    \n",
    "    \n",
    "# Download data if needed\n",
    "url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "# training data\n",
    "maybe_download(url,'train-images-idx3-ubyte.gz',9912422)\n",
    "maybe_download(url,'train-labels-idx1-ubyte.gz',28881)\n",
    "# testing data\n",
    "maybe_download(url,'t10k-images-idx3-ubyte.gz',1648877)\n",
    "maybe_download(url,'t10k-labels-idx1-ubyte.gz',4542)\n",
    "\n",
    "# Read the training and testing data \n",
    "train_inputs, train_labels = read_mnist('train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz',True)\n",
    "test_inputs, test_labels = read_mnist('t10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz',True)\n",
    "\n",
    "print('\\nTrain size: ', train_inputs.shape[0])\n",
    "print('\\nTest size: ', test_inputs.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model\n",
    "Here we define the CNN, we first define set of convolution layers (`Conv2D`) and max-pooling layers (`MaxPool2D`) and then two fully-connected layers (`Dense`). You can see that the convolution and max-pooling layers are interleaved and the fully-connected layers sit at the end. Finally we compile the model, with the cross entropy loss and an optimizer, that will train your model. We will also monitor the accuracy during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5abf96bc65da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "config.gpu_options.allow_growth = True \n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5c5220b9be0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Define a sequential model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Added a convolution layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Added a convolution layer\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=[28, 28, 1]))\n",
    "\n",
    "# Add a max pool lyer\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "# Convolution layer\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "# Max pool layer\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "# More convolution, max pool\n",
    "model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Algorithm: Training and Testing\n",
    "\n",
    "Running training and testing is quite easy, as long as we have the training/testing , input/output data loaded. It follows scikit-like syntax to implementn model fitting and evaulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch:  0\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5 # Number of epochs the training runs for\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "x_train, y_train = train_inputs, train_labels\n",
    "x_test, y_test = test_inputs, test_labels\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    print('Training for epoch: ',epoch)    \n",
    "    \n",
    "    # Training for a single epoch (scikit-like syntax)\n",
    "    model.fit(x_train, y_train, batch_size = batch_size)\n",
    "    \n",
    "    # Testing phase\n",
    "    test_acc = model.evaluate(x_test, y_test, batch_size=batch_size)  \n",
    "    print('\\tEpoch (', epoch ,') Test accuracy: ',test_acc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
