{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NMT with Tensorflow seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Seq2Seq Items\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining some hyperparameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size= 50000\n",
    "num_units = 128\n",
    "input_size = 128\n",
    "batch_size = 16\n",
    "source_sequence_length=40\n",
    "target_sequence_length=60\n",
    "decoder_type = 'basic' # could be basic or attention\n",
    "sentences_to_read = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "\t [('anscheinend', 6939), ('vordefinierte', 34980), ('kräftig', 15399), ('Leidenschaften', 27964), ('Welthandels', 14859), ('Überarbeitung', 4929), ('Lacona', 46060), ('konkurrenzfähig', 24079), ('Gemeinschaftsmarkt', 44149), ('auslesen', 42018)]\n",
      "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, ','), (4, '.'), (5, 'die'), (6, 'der'), (7, 'und'), (8, 'in'), (9, 'zu')]\n",
      "\t Vocabulary size:  50000\n",
      "Target\n",
      "\t [('Lacona', 30798), ('extravagance', 44093), ('www.youtube.com', 42573), ('Eurocrats', 33031), ('Mausoleum', 44704), ('costumers', 31164), ('idyll', 37266), ('clerical', 30039), ('det', 15409), ('Flemming', 26915)]\n",
      "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, 'the'), (4, ','), (5, '.'), (6, 'of'), (7, 'and'), (8, 'to'), (9, 'in')]\n",
      "\t Vocabulary size:  50000\n"
     ]
    }
   ],
   "source": [
    "src_dictionary = dict()\n",
    "with open('vocab.50K.de.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        #we are discarding last char as it is new line char\n",
    "        src_dictionary[line[:-1]] = len(src_dictionary)\n",
    "\n",
    "src_reverse_dictionary = dict(zip(src_dictionary.values(),src_dictionary.keys()))\n",
    "\n",
    "print('Source')\n",
    "print('\\t',list(src_dictionary.items())[:10])\n",
    "print('\\t',list(src_reverse_dictionary.items())[:10])\n",
    "print('\\t','Vocabulary size: ', len(src_dictionary))\n",
    "\n",
    "tgt_dictionary = dict()\n",
    "with open('vocab.50K.en.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        #we are discarding last char as it is new line char\n",
    "        tgt_dictionary[line[:-1]] = len(tgt_dictionary)\n",
    "\n",
    "tgt_reverse_dictionary = dict(zip(tgt_dictionary.values(),tgt_dictionary.keys()))\n",
    "\n",
    "print('Target')\n",
    "print('\\t',list(tgt_dictionary.items())[:10])\n",
    "print('\\t',list(tgt_reverse_dictionary.items())[:10])\n",
    "print('\\t','Vocabulary size: ', len(tgt_dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Sentences (English and German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample translations (50000)\n",
      "( 0 ) DE:  Heute verstehen sich QuarkXPress ® 8 , Photoshop ® und Illustrator ® besser als jemals zuvor . Dank HTML und CSS ­ können Anwender von QuarkXPress inzwischen alle Medien bedienen , und das unabhängig von Anwendungen der Adobe ® Creative Suite ® wie Adobe Flash ® ( SWF ) und Adobe Dreamweaver ® .\n",
      "\n",
      "( 0 ) EN:  Today , QuarkXPress ® 8 has tighter integration with Photoshop ® and Illustrator ® than ever before , and through standards like HTML and CSS , QuarkXPress users can publish across media both independently and alongside Adobe ® Creative Suite ® applications like Adobe Flash ® ( SWF ) and Adobe Dreamweaver ® .\n",
      "\n",
      "( 10000 ) DE:  Es existieren Busverbindungen in nahezu jeden Ort der Provence ( eventuell mit Umsteigen in Aix ##AT##-##AT## en ##AT##-##AT## Provence ) , allerdings sollte beachtet werden , dass die letzten Busse abends ca. um 19 Uhr fahren .\n",
      "\n",
      "( 10000 ) EN:  As always in France those highways are expensive but practical , comfortable and fast .\n",
      "\n",
      "( 20000 ) DE:  Es war staubig , das Bad schmutzig . Sogar die Beleuchtung an der Wand im Flur ( Seitengebäude ) war richtig verstaubt .\n",
      "\n",
      "( 20000 ) EN:  It was rather old fashioned in the decoration .\n",
      "\n",
      "( 30000 ) DE:  Auch ist , so denkt Dr. Gutherz , bereits die erste Seite sehr viel versprechend , da sie eine Definition des klinischen Psychotrauma ##AT##-##AT## Begriffes enthält , der er gänzlich zustimmen kann .\n",
      "\n",
      "( 30000 ) EN:  At the rhetorical climax of this summary , Dr Goodheart comes across some sentences expressed with great pathos .\n",
      "\n",
      "( 40000 ) DE:  Bei einer digitalen Bildkette wird das Intensitätssignal für jedes Pixel ohne analoge Zwischenschritte direkt in der Detektoreinheit digitalisiert , d.h. in Zahlen umgewandelt .\n",
      "\n",
      "( 40000 ) EN:  A digital image chain is an image chain that is equipped with a digital detector instead of an analogue one .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_sent = []\n",
    "target_sent = []\n",
    "\n",
    "test_source_sent = []\n",
    "test_target_sent = []\n",
    "\n",
    "\n",
    "with open('train.de', encoding='utf-8') as f:\n",
    "    for l_i, line in enumerate(f):\n",
    "        # discarding first 20 translations as there was some\n",
    "        # english to english translations found in the first few. which are wrong\n",
    "        if l_i<50:\n",
    "            continue\n",
    "        source_sent.append(line)\n",
    "        if len(source_sent)>=sentences_to_read:\n",
    "            break\n",
    "        \n",
    "            \n",
    "with open('train.en', encoding='utf-8') as f:\n",
    "    for l_i, line in enumerate(f):\n",
    "        if l_i<50:\n",
    "            continue\n",
    "        \n",
    "        target_sent.append(line)\n",
    "        if len(target_sent)>=sentences_to_read:\n",
    "            break\n",
    "        \n",
    "            \n",
    "assert len(source_sent)==len(target_sent),'Source: %d, Target: %d'%(len(source_sent),len(target_sent))\n",
    "\n",
    "print('Sample translations (%d)'%len(source_sent))\n",
    "for i in range(0,sentences_to_read,10000):\n",
    "    print('(',i,') DE: ', source_sent[i])\n",
    "    print('(',i,') EN: ', target_sent[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's analyse some statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Source) Sentence mean length:  26.35934\n",
      "(Source) Sentence stddev length:  13.9681614669\n",
      "(Target) Sentence mean length:  28.58758\n",
      "(Target) Sentence stddev length:  15.1544201388\n"
     ]
    }
   ],
   "source": [
    "def split_to_tokens(sent,is_source):\n",
    "    #sent = sent.replace('-',' ')\n",
    "    sent = sent.replace(',',' ,')\n",
    "    sent = sent.replace('.',' .')\n",
    "    sent = sent.replace('\\n',' ') \n",
    "    \n",
    "    sent_toks = sent.split(' ')\n",
    "    for t_i, tok in enumerate(sent_toks):\n",
    "        if is_source:\n",
    "            if tok not in src_dictionary.keys():\n",
    "                sent_toks[t_i] = '<unk>'\n",
    "        else:\n",
    "            if tok not in tgt_dictionary.keys():\n",
    "                sent_toks[t_i] = '<unk>'\n",
    "    return sent_toks\n",
    "\n",
    "# Let us first look at some statistics of the sentences\n",
    "source_len = []\n",
    "source_mean, source_std = 0,0\n",
    "for sent in source_sent:\n",
    "    source_len.append(len(split_to_tokens(sent,True)))\n",
    "\n",
    "print('(Source) Sentence mean length: ', np.mean(source_len))\n",
    "print('(Source) Sentence stddev length: ', np.std(source_len))\n",
    "\n",
    "target_len = []\n",
    "target_mean, target_std = 0,0\n",
    "for sent in target_sent:\n",
    "    target_len.append(len(split_to_tokens(sent,False)))\n",
    "\n",
    "print('(Target) Sentence mean length: ', np.mean(target_len))\n",
    "print('(Target) Sentence stddev length: ', np.std(target_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the special tokens and make all sentences same length (for batch-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sent lengths:  121\n",
      "Samples from bin\n",
      "\t ['<s>', 'Heute', 'verstehen', 'sich', 'QuarkXPress', '®', '8', '<unk>', ',', 'Photoshop', '®', 'und', 'Illustrator', '®', 'besser', 'als', 'jemals', 'zuvor', '<unk>', '.', 'Dank', 'HTML', 'und', 'CSS', '\\xad', 'können', 'Anwender', 'von', 'QuarkXPress', 'inzwischen', 'alle', 'Medien', 'bedienen', '<unk>', ',', 'und', 'das', 'unabhängig', 'von', 'Anwendungen', 'der']\n",
      "\t ['</s>', 'Today', '<unk>', ',', 'QuarkXPress', '®', '8', 'has', 'tighter', 'integration', 'with', 'Photoshop', '®', 'and', 'Illustrator', '®', 'than', 'ever', 'before', '<unk>', ',', 'and', 'through', 'standards', 'like', 'HTML', 'and', 'CSS', '<unk>', ',', 'QuarkXPress', 'users', 'can', 'publish', 'across', 'media', 'both', 'independently', 'and', 'alongside', 'Adobe', '®', 'Creative', 'Suite', '®', 'applications', 'like', 'Adobe', 'Flash', '®', '(', 'SWF', ')', 'and', 'Adobe', 'Dreamweaver', '®', '<unk>', '.', '<unk>', '</s>']\n",
      "\t ['<s>', 'Erstellen', 'Sie', 'einen', 'Rahmen', 'und', 'gehen', 'Sie', 'dann', 'auf', 'Datei', '&gt;', 'Importieren', '<unk>', '.', '.', '.', 'oder', 'ziehen', 'Sie', 'das', 'Bild', 'einfach', 'per', 'Drag', '&amp;', 'Drop', 'von', 'Ihrem', 'Desktop', '<unk>', ',', 'aus', 'dem', 'Finder', 'oder', 'einer', 'Anwendung', 'wie', 'Adobe', 'Bridge']\n",
      "\t ['</s>', 'Bringing', 'the', 'PSD', 'files', 'into', 'QuarkXPress', 'is', 'the', 'same', 'as', 'any', 'other', 'image', '<unk>', '.', 'Create', 'a', 'Box', 'and', 'then', 'use', 'File', '&gt;', 'Import', '<unk>', '.', '.', '.', 'or', 'simply', 'drag', 'and', 'drop', 'the', 'image', 'from', 'your', 'desktop', '<unk>', ',', 'Finder', 'or', 'an', 'application', 'like', 'Adobe', 'Bridge', '®', 'with', 'or', 'without', 'creating', 'a', 'box', 'first', '<unk>', '.', '<unk>', '</s>', '</s>']\n",
      "\n",
      "\tSentences  50000\n"
     ]
    }
   ],
   "source": [
    "train_inputs = []\n",
    "train_outputs = []\n",
    "train_inp_lengths = []\n",
    "train_out_lengths = []\n",
    "\n",
    "max_tgt_sent_lengths = 0\n",
    "\n",
    "src_max_sent_length = 41\n",
    "tgt_max_sent_length = 61\n",
    "for s_i, (src_sent, tgt_sent) in enumerate(zip(source_sent,target_sent)):\n",
    "    \n",
    "    src_sent_tokens = split_to_tokens(src_sent,True)\n",
    "    tgt_sent_tokens = split_to_tokens(tgt_sent,False)\n",
    "        \n",
    "    num_src_sent = []\n",
    "    for tok in src_sent_tokens:\n",
    "        num_src_sent.append(src_dictionary[tok])\n",
    "\n",
    "    num_src_set = num_src_sent[::-1] # we reverse the source sentence. This improves performance\n",
    "    num_src_sent.insert(0,src_dictionary['<s>'])\n",
    "    train_inp_lengths.append(min(len(num_src_sent)+1,src_max_sent_length))\n",
    "    \n",
    "    # append until the sentence reaches max length\n",
    "    if len(num_src_sent)<src_max_sent_length:\n",
    "        num_src_sent.extend([src_dictionary['</s>'] for _ in range(src_max_sent_length - len(num_src_sent))])\n",
    "    # if more than max length, truncate the sentence\n",
    "    elif len(num_src_sent)>src_max_sent_length:\n",
    "        num_src_sent = num_src_sent[:src_max_sent_length]\n",
    "    assert len(num_src_sent)==src_max_sent_length,len(num_src_sent)\n",
    "\n",
    "    train_inputs.append(num_src_sent)\n",
    "\n",
    "    num_tgt_sent = [tgt_dictionary['</s>']]\n",
    "    for tok in tgt_sent_tokens:\n",
    "        num_tgt_sent.append(tgt_dictionary[tok])\n",
    "    \n",
    "    train_out_lengths.append(min(len(num_tgt_sent)+1,tgt_max_sent_length))\n",
    "    \n",
    "    if len(num_tgt_sent)<tgt_max_sent_length:\n",
    "        num_tgt_sent.extend([tgt_dictionary['</s>'] for _ in range(tgt_max_sent_length - len(num_tgt_sent))])\n",
    "    elif len(num_tgt_sent)>tgt_max_sent_length:\n",
    "        num_tgt_sent = num_tgt_sent[:tgt_max_sent_length]\n",
    "    \n",
    "    train_outputs.append(num_tgt_sent)\n",
    "    assert len(train_outputs[s_i])==tgt_max_sent_length, 'Sent length needs to be 60, but is %d'%len(binned_outputs[s_i])    \n",
    "\n",
    "assert len(train_inputs)  == len(source_sent),\\\n",
    "        'Size of total bin elements: %d, Total sentences: %d'\\\n",
    "                %(len(train_inputs),len(source_sent))\n",
    "\n",
    "print('Max sent lengths: ', max_tgt_sent_lengths)\n",
    "\n",
    "\n",
    "train_inputs = np.array(train_inputs, dtype=np.int32)\n",
    "train_outputs = np.array(train_outputs, dtype=np.int32)\n",
    "train_inp_lengths = np.array(train_inp_lengths, dtype=np.int32)\n",
    "train_out_lengths = np.array(train_out_lengths, dtype=np.int32)\n",
    "print('Samples from bin')\n",
    "print('\\t',[src_reverse_dictionary[w]  for w in train_inputs[0,:].tolist()])\n",
    "print('\\t',[tgt_reverse_dictionary[w]  for w in train_outputs[0,:].tolist()])\n",
    "print('\\t',[src_reverse_dictionary[w]  for w in train_inputs[10,:].tolist()])\n",
    "print('\\t',[tgt_reverse_dictionary[w]  for w in train_outputs[10,:].tolist()])\n",
    "print()\n",
    "print('\\tSentences ',train_inputs.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data\n",
      "['Heute', 'Hier', 'Sie', 'Häufig', 'In']\n",
      "['verstehen', 'erfahren', 'werden', 'wird', 'diesem']\n",
      "['sich', 'Sie', 'überrascht', 'die', 'Abschnitt']\n",
      "['QuarkXPress', '<unk>', 'sein', 'Meinung', 'erläutern']\n",
      "['®', ',', '<unk>', 'vertreten', 'wir']\n",
      "['8', 'wie', ',', '<unk>', '<unk>']\n",
      "['<unk>', 'Sie', 'wie', ',', ',']\n",
      "[',', 'Creative', 'einfach', 'dass', 'wann']\n",
      "['Photoshop', 'Suite', 'sich', 'QuarkXPress', 'Sie']\n",
      "['®', '2', 'mit', '8', 'für']\n",
      "['und', 'und', 'Quark', 'von', 'Ihre']\n",
      "['Illustrator', 'Creative', 'das', 'allen', 'Bilder']\n",
      "['®', 'Suite', 'volle', 'heute', 'das']\n",
      "['besser', '3', 'Potenzial', 'verfügbaren', 'PSD']\n",
      "['als', 'am', 'Ihrer', 'Layout', '##AT##-##AT##']\n",
      "['jemals', 'besten', 'Design', '##AT##-##AT##', 'Format']\n",
      "['zuvor', 'zusammen', '##AT##-##AT##', 'Programmen', 'verwenden']\n",
      "['<unk>', 'mit', 'Software', 'die', 'sollten']\n",
      "['.', 'QuarkXPress', 'erschließen', 'beste', 'und']\n",
      "['Dank', 'nutzen', 'lässt', 'Integration', 'wie']\n",
      "['HTML', 'können', '<unk>', 'mit', 'Sie']\n",
      "['und', '<unk>', '.', 'Photoshop', 'es']\n",
      "['CSS', '.', '<unk>', 'über', 'für']\n",
      "['\\xad', '<unk>', '</s>', 'das', 'Ihre']\n",
      "['können', '</s>', '</s>', 'PSD', 'Bilder']\n",
      "['Anwender', '</s>', '</s>', '##AT##-##AT##', 'optimal']\n",
      "['von', '</s>', '</s>', 'Dateiformat', 'nutzen']\n",
      "['QuarkXPress', '</s>', '</s>', 'bietet', '<unk>']\n",
      "['inzwischen', '</s>', '</s>', '<unk>', '.']\n",
      "['alle', '</s>', '</s>', '.', '<unk>']\n",
      "['Medien', '</s>', '</s>', '<unk>', '</s>']\n",
      "['bedienen', '</s>', '</s>', '</s>', '</s>']\n",
      "['<unk>', '</s>', '</s>', '</s>', '</s>']\n",
      "[',', '</s>', '</s>', '</s>', '</s>']\n",
      "['und', '</s>', '</s>', '</s>', '</s>']\n",
      "['das', '</s>', '</s>', '</s>', '</s>']\n",
      "['unabhängig', '</s>', '</s>', '</s>', '</s>']\n",
      "['von', '</s>', '</s>', '</s>', '</s>']\n",
      "['Anwendungen', '</s>', '</s>', '</s>', '</s>']\n",
      "['der', '</s>', '</s>', '</s>', '</s>']\n",
      "\n",
      "Target data batch (first time)\n",
      "['Today', 'You', 'QuarkXPress', 'In', 'For']\n",
      "['<unk>', '’', '8', 'this', 'example']\n",
      "[',', 'll', 'is', 'section', '<unk>']\n",
      "['QuarkXPress', 'be', 'considered', 'we', ',']\n",
      "['®', 'surprised', 'by', '’', 'you']\n",
      "['8', 'how', 'many', 'll', 'may']\n",
      "['has', 'easy', 'to', 'explain', 'have']\n",
      "['tighter', 'Quark', 'have', 'when', 'multiple']\n",
      "['integration', 'has', 'the', 'you', 'layers']\n",
      "['with', 'made', 'best', 'should', 'in']\n",
      "['Photoshop', 'it', 'integration', 'use', 'your']\n",
      "['®', 'to', 'with', 'the', 'PSD']\n",
      "['and', 'unlock', 'Photoshop', 'PSD', 'with']\n",
      "['Illustrator', 'the', '’', 'format', 'different']\n",
      "['®', 'full', 's', 'for', 'product']\n",
      "['than', 'potential', 'PSD', 'your', 'shots']\n",
      "['ever', 'of', 'file', 'images', '<unk>']\n",
      "['before', 'all', 'format', 'and', ',']\n",
      "['<unk>', 'your', 'of', 'how', 'which']\n",
      "[',', 'design', 'any', 'to', 'will']\n",
      "['and', 'software', 'layout', 'get', 'vary']\n",
      "['through', '<unk>', 'tool', 'the', 'from']\n",
      "['standards', '.', 'available', 'most', 'publication']\n",
      "['like', '<unk>', 'today', 'out', 'to']\n",
      "['HTML', '</s>', '<unk>', 'of', 'publication']\n",
      "['and', '</s>', '.', 'them', '<unk>']\n",
      "['CSS', '</s>', '<unk>', '<unk>', '.']\n",
      "['<unk>', '</s>', '</s>', '.', '<unk>']\n",
      "[',', '</s>', '</s>', '<unk>', '</s>']\n",
      "['QuarkXPress', '</s>', '</s>', '</s>', '</s>']\n",
      "['users', '</s>', '</s>', '</s>', '</s>']\n",
      "['can', '</s>', '</s>', '</s>', '</s>']\n",
      "['publish', '</s>', '</s>', '</s>', '</s>']\n",
      "['across', '</s>', '</s>', '</s>', '</s>']\n",
      "['media', '</s>', '</s>', '</s>', '</s>']\n",
      "['both', '</s>', '</s>', '</s>', '</s>']\n",
      "['independently', '</s>', '</s>', '</s>', '</s>']\n",
      "['and', '</s>', '</s>', '</s>', '</s>']\n",
      "['alongside', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['Creative', '</s>', '</s>', '</s>', '</s>']\n",
      "['Suite', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['applications', '</s>', '</s>', '</s>', '</s>']\n",
      "['like', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['Flash', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['(', '</s>', '</s>', '</s>', '</s>']\n",
      "['SWF', '</s>', '</s>', '</s>', '</s>']\n",
      "[')', '</s>', '</s>', '</s>', '</s>']\n",
      "['and', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['Dreamweaver', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['<unk>', '</s>', '</s>', '</s>', '</s>']\n",
      "['.', '</s>', '</s>', '</s>', '</s>']\n",
      "['<unk>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "\n",
      "Target data batch (non-first time)\n",
      "['Today', 'You', 'QuarkXPress', 'In', 'For']\n",
      "['<unk>', '’', '8', 'this', 'example']\n",
      "[',', 'll', 'is', 'section', '<unk>']\n",
      "['QuarkXPress', 'be', 'considered', 'we', ',']\n",
      "['®', 'surprised', 'by', '’', 'you']\n",
      "['8', 'how', 'many', 'll', 'may']\n",
      "['has', 'easy', 'to', 'explain', 'have']\n",
      "['tighter', 'Quark', 'have', 'when', 'multiple']\n",
      "['integration', 'has', 'the', 'you', 'layers']\n",
      "['with', 'made', 'best', 'should', 'in']\n",
      "['Photoshop', 'it', 'integration', 'use', 'your']\n",
      "['®', 'to', 'with', 'the', 'PSD']\n",
      "['and', 'unlock', 'Photoshop', 'PSD', 'with']\n",
      "['Illustrator', 'the', '’', 'format', 'different']\n",
      "['®', 'full', 's', 'for', 'product']\n",
      "['than', 'potential', 'PSD', 'your', 'shots']\n",
      "['ever', 'of', 'file', 'images', '<unk>']\n",
      "['before', 'all', 'format', 'and', ',']\n",
      "['<unk>', 'your', 'of', 'how', 'which']\n",
      "[',', 'design', 'any', 'to', 'will']\n",
      "['and', 'software', 'layout', 'get', 'vary']\n",
      "['through', '<unk>', 'tool', 'the', 'from']\n",
      "['standards', '.', 'available', 'most', 'publication']\n",
      "['like', '<unk>', 'today', 'out', 'to']\n",
      "['HTML', '</s>', '<unk>', 'of', 'publication']\n",
      "['and', '</s>', '.', 'them', '<unk>']\n",
      "['CSS', '</s>', '<unk>', '<unk>', '.']\n",
      "['<unk>', '</s>', '</s>', '.', '<unk>']\n",
      "[',', '</s>', '</s>', '<unk>', '</s>']\n",
      "['QuarkXPress', '</s>', '</s>', '</s>', '</s>']\n",
      "['users', '</s>', '</s>', '</s>', '</s>']\n",
      "['can', '</s>', '</s>', '</s>', '</s>']\n",
      "['publish', '</s>', '</s>', '</s>', '</s>']\n",
      "['across', '</s>', '</s>', '</s>', '</s>']\n",
      "['media', '</s>', '</s>', '</s>', '</s>']\n",
      "['both', '</s>', '</s>', '</s>', '</s>']\n",
      "['independently', '</s>', '</s>', '</s>', '</s>']\n",
      "['and', '</s>', '</s>', '</s>', '</s>']\n",
      "['alongside', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['Creative', '</s>', '</s>', '</s>', '</s>']\n",
      "['Suite', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['applications', '</s>', '</s>', '</s>', '</s>']\n",
      "['like', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['Flash', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['(', '</s>', '</s>', '</s>', '</s>']\n",
      "['SWF', '</s>', '</s>', '</s>', '</s>']\n",
      "[')', '</s>', '</s>', '</s>', '</s>']\n",
      "['and', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['Dreamweaver', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['<unk>', '</s>', '</s>', '</s>', '</s>']\n",
      "['.', '</s>', '</s>', '</s>', '</s>']\n",
      "['<unk>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "input_size = 128\n",
    "\n",
    "class DataGeneratorMT(object):\n",
    "    \n",
    "    def __init__(self,batch_size,num_unroll,is_source):\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        \n",
    "        \n",
    "        self._src_word_embeddings = np.load('de-embeddings.npy')\n",
    "        \n",
    "        self._tgt_word_embeddings = np.load('en-embeddings.npy')\n",
    "        \n",
    "        self._sent_ids = None\n",
    "        \n",
    "        self._is_source = is_source\n",
    "        \n",
    "                \n",
    "    def next_batch(self, sent_ids, first_set):\n",
    "        \n",
    "        if self._is_source:\n",
    "            max_sent_length = src_max_sent_length\n",
    "        else:\n",
    "            max_sent_length = tgt_max_sent_length\n",
    "        batch_labels_ind = []\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        \n",
    "        for b in range(self._batch_size):\n",
    "            \n",
    "            sent_id = sent_ids[b]\n",
    "            \n",
    "            if self._is_source:\n",
    "                sent_text = train_inputs[sent_id]\n",
    "                             \n",
    "                batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b]=sent_text[self._cursor[b]+1]\n",
    "\n",
    "            else:\n",
    "                sent_text = train_outputs[sent_id]\n",
    "                \n",
    "                # We cannot avoid having two different embedding vectors for <s> token\n",
    "                # in soruce and target languages\n",
    "                # Therefore, if the symbol appears, we always take the source embedding vector\n",
    "                if sent_text[self._cursor[b]]!=src_dictionary['<s>']:\n",
    "                    batch_data[b] = sent_text[self._cursor[b]]\n",
    "                else:\n",
    "                    batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b] = sent_text[self._cursor[b]+1]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%(max_sent_length-1)\n",
    "                                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self,sent_ids):\n",
    "        \n",
    "        if sent_ids is not None:\n",
    "            \n",
    "            self._sent_ids = sent_ids\n",
    "            \n",
    "            #if self._is_source:\n",
    "                # we dont star at the very beginning, becaues the very beginning is a bunch of </s> symbols.\n",
    "                # so we start from the middel s.t we get a minimum number of </s> symbols in our training data\n",
    "                # this is only needed for source language\n",
    "                #self._cursor = ((start_indices_for_bins[bin_id][self._sent_ids]//self._num_unroll)*self._num_unroll).tolist()\n",
    "            #else:\n",
    "            self._cursor = [0 for _ in range(self._batch_size)]\n",
    "                \n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        inp_lengths = None\n",
    "        for ui in range(self._num_unroll):\n",
    "            # The first batch in any batch of captions is different\n",
    "            if self._is_source:\n",
    "                data, labels = self.next_batch(self._sent_ids, False)\n",
    "            else:\n",
    "                data, labels = self.next_batch(self._sent_ids, False)\n",
    "                    \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "            inp_lengths = train_inp_lengths[sent_ids]\n",
    "        return unroll_data, unroll_labels, self._sent_ids, inp_lengths\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if the implementation correct\n",
    "dg = DataGeneratorMT(batch_size=5,num_unroll=40,is_source=True)\n",
    "u_data, u_labels, _, _ = dg.unroll_batches([0,1,2,3,4])\n",
    "\n",
    "print('Source data')\n",
    "for _, lbl in zip(u_data,u_labels):\n",
    "    print([src_reverse_dictionary[w] for w in lbl.tolist()])\n",
    "\n",
    "        \n",
    "# Running a tiny set to see if the implementation correct\n",
    "dg = DataGeneratorMT(batch_size=5,num_unroll=60,is_source=False)\n",
    "u_data, u_labels, _, _ = dg.unroll_batches([0,2,3,4,5])\n",
    "print('\\nTarget data batch (first time)')\n",
    "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
    "    #if d_i>5 and d_i < 35:\n",
    "    #    continue\n",
    "\n",
    "    print([tgt_reverse_dictionary[w] for w in lbl.tolist()])\n",
    "\n",
    "print('\\nTarget data batch (non-first time)')\n",
    "u_data, u_labels, _, _ = dg.unroll_batches(None)\n",
    "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
    "    \n",
    "    #if d_i>5 and d_i < 35:\n",
    "    #    continue\n",
    "        \n",
    "    print([tgt_reverse_dictionary[w] for w in lbl.tolist()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs Outputs Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "enc_train_inputs = []\n",
    "dec_train_inputs = []\n",
    "\n",
    "# Need to use pre-trained word embeddings\n",
    "encoder_emb_layer = tf.convert_to_tensor(np.load('de-embeddings.npy'))\n",
    "decoder_emb_layer = tf.convert_to_tensor(np.load('en-embeddings.npy'))\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(source_sequence_length):\n",
    "    enc_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size],name='enc_train_inputs_%d'%ui))\n",
    "\n",
    "dec_train_labels=[]\n",
    "dec_label_masks = []\n",
    "for ui in range(target_sequence_length):\n",
    "    dec_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size],name='dec_train_inputs_%d'%ui))\n",
    "    dec_train_labels.append(tf.placeholder(tf.int32, shape=[batch_size],name='dec-train_outputs_%d'%ui))\n",
    "    dec_label_masks.append(tf.placeholder(tf.float32, shape=[batch_size],name='dec-label_masks_%d'%ui))\n",
    "    \n",
    "encoder_emb_inp = [tf.nn.embedding_lookup(encoder_emb_layer, src) for src in enc_train_inputs]\n",
    "encoder_emb_inp = tf.stack(encoder_emb_inp)\n",
    "\n",
    "decoder_emb_inp = [tf.nn.embedding_lookup(decoder_emb_layer, src) for src in dec_train_inputs]\n",
    "decoder_emb_inp = tf.stack(decoder_emb_inp)\n",
    "\n",
    "enc_train_inp_lengths = tf.placeholder(tf.int32, shape=[batch_size],name='train_input_lengths')\n",
    "dec_train_inp_lengths = tf.placeholder(tf.int32, shape=[batch_size],name='train_output_lengths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_emb_inp, initial_state=initial_state,\n",
    "    sequence_length=enc_train_inp_lengths, \n",
    "    time_major=True, swap_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RNN cell\n",
    "decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "projection_layer = Dense(units=vocab_size, use_bias=True)\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_emb_inp, [tgt_max_sent_length-1 for _ in range(batch_size)], time_major=True)\n",
    "\n",
    "# Decoder\n",
    "if decoder_type == 'basic':\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, encoder_state,\n",
    "        output_layer=projection_layer)\n",
    "    \n",
    "elif decoder_type == 'attention':\n",
    "    decoder = tf.contrib.seq2seq.BahdanauAttention(\n",
    "        decoder_cell, helper, encoder_state,\n",
    "        output_layer=projection_layer)\n",
    "    \n",
    "# Dynamic decoding\n",
    "outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, output_time_major=True,\n",
    "    swap_memory=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = outputs.rnn_output\n",
    "\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=dec_train_labels, logits=logits)\n",
    "loss = (tf.reduce_sum(crossent*tf.stack(dec_label_masks)) / (batch_size*target_sequence_length))\n",
    "\n",
    "train_prediction = outputs.sample_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Optimizer with Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Optimizer\n"
     ]
    }
   ],
   "source": [
    "print('Defining Optimizer')\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    0.01, global_step, decay_steps=10, decay_rate=0.9, staircase=True)\n",
    "\n",
    "with tf.variable_scope('SGD'):\n",
    "    sgd_optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "sgd_gradients, v = zip(*sgd_optimizer.compute_gradients(loss))\n",
    "sgd_gradients, _ = tf.clip_by_global_norm(sgd_gradients, 25.0)\n",
    "sgd_optimize = optimizer.apply_gradients(zip(sgd_gradients, v))\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  250\n",
      "Actual: To find the nearest car park to an apartment <unk> , have a look at this map link <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The the the hotel of <unk> <unk> the <unk> <unk> , the the <unk> <unk> the <unk> <unk> <unk> , <unk> </s> \n",
      "\n",
      "\n",
      "Actual: You can enjoy a buffet breakfast every morning from 07 : 30 to 10 : 30 <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The <unk> the the <unk> <unk> <unk> <unk> <unk> the <unk> <unk> <unk> the <unk> <unk> <unk> . <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  500\n",
      "Actual: When would you like to stay at the <unk> ? <unk> </s> \n",
      "\n",
      "Predicted: The you you like to stay at the <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: When would you like to stay at the Barceló Malaga ? <unk> </s> \n",
      "\n",
      "Predicted: The you you like to stay at the <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  500  =============\n",
      "\t Loss:  2.46085503793\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  750\n",
      "Actual: Simply complete our checklist and we will contact you with the tailor ##AT##-##AT## made services you require <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The the <unk> <unk> of the are be the <unk> the <unk> <unk> <unk> <unk> <unk> <unk> the . <unk> </s> \n",
      "\n",
      "\n",
      "Actual: For more information on I / O availability <unk> , please reference the related links below <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The the <unk> <unk> the <unk> <unk> <unk> <unk> , and <unk> <unk> <unk> to to <unk> , <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  1000\n",
      "Actual: <unk> between peoples and the healthy effects of hiking are the primary topics <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The <unk> the of the <unk> ##AT##-##AT## of the <unk> a <unk> <unk> of , <unk> </s> \n",
      "\n",
      "\n",
      "Actual: All rooms are in <unk> Island designed with panoramic sea view under the mountains <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The older are available the <unk> <unk> <unk> the ##AT##-##AT## <unk> <unk> the hotel of . <unk> </s> \n",
      "\n",
      "============= Step  1000  =============\n",
      "\t Loss:  2.07367780662\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  1250\n",
      "Actual: The 3 <unk> m ² <unk> Spa is the ultimate setting for an active &amp; relaxing vacation for all of the senses <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The hotel ##AT##-##AT## , ² <unk> , <unk> a total <unk> for the excellent <unk> room atmosphere <unk> the <unk> the room <unk> . <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Recommended and required patches are hosted for download on the <unk> website <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The <unk> the to to charged by the <unk> the <unk> <unk> <unk> . <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  1500\n",
      "Actual: It is clearly evident that athletes benefit from Palatinose ™ ( <unk> ) in two ways : on the one hand <unk> , it provides long ##AT##-##AT## lasting energy in the form of glucose <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The is a as to the to from the to <unk> <unk> ) <unk> the ##AT##-##AT## of <unk> the <unk> of <unk> . and is a <unk> free ##AT##-##AT## <unk> the <unk> of the <unk> . <unk> </s> \n",
      "\n",
      "\n",
      "Actual: This ideally located new hotel ( opened in November 2007 ) combines its proximity to Vienna and Vienna Airport with excellent facilities and comfortable rooms <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The page is hotel hotel is hotel in the distance <unk> <unk> the excellent to the <unk> the <unk> <unk> the <unk> <unk> a and <unk> . <unk> </s> \n",
      "\n",
      "============= Step  1500  =============\n",
      "\t Loss:  1.94425997424\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  1750\n",
      "Actual: The Ponte Vecchio Suites are part of an innovative structure inspired by the &quot; boutique hotel &quot; concept and it is precisely in this building that you will find the modern SPA <unk> , intended for the wellbeing <unk> , relaxation and exclusive use of our guests <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The hotel <unk> <unk> <unk> a of the elegant <unk> of by the <unk> Trade <unk> <unk> <unk> <unk> the &apos;s a in the <unk> <unk> is can find a most <unk> <unk> . the to the <unk> of . and and restaurants <unk> of the most <unk> . <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Eric J . <unk> <unk> &quot; the <unk> of women and the mobility of men &quot; as one of &quot; certain realities in the history of travel &quot; <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The <unk> <unk> <unk> <unk> , <unk> <unk> <unk> the <unk> the <unk> of the <unk> <unk> a of the <unk> &quot; <unk> the world of the <unk> <unk> . <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  2000\n",
      "Actual: Or <unk> , click one of the Browse buttons and find the content yourself <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The you , the on of the <unk> <unk> <unk> the the <unk> of <unk> . <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Also learn how engineers implement <unk> on <unk> <unk> . <unk> Schneider <unk> , Senior R &amp; D Manager <unk> , <unk> Systems Joe <unk> <unk> , Senior Engineer <unk> , <unk> . . . <unk> </s> \n",
      "\n",
      "Predicted: The <unk> to to to to , the , , <unk> </s> \n",
      "\n",
      "============= Step  2000  =============\n",
      "\t Loss:  1.86829733658\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  2250\n",
      "Actual: The 5 ##AT##-##AT## star Radisson Plaza Sydney is a beautifully furnished hotel located in a stunning heritage listed building in the heart of the city surrounded by 75 of <unk> top 500 companies <unk> , famous <unk> . . . <unk> </s> \n",
      "\n",
      "Predicted: The hotel ##AT##-##AT## star hotel is Hotel is situated great renovated and with in the heart building <unk> building <unk> the centre of the city <unk> by the minutes the <unk> <unk> metres <unk> . and <unk> , <unk> . <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Free Download : Get Bluetooth Driver Now <unk> . Official Bluetooth Drivers ! <unk> </s> \n",
      "\n",
      "Predicted: <unk> internet ##AT##-##AT## The the ##AT##-##AT## <unk> <unk> , <unk> <unk> is <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  2500\n",
      "Actual: The remaining balance is to be paid upon arrival <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The hotel <unk> of the be found <unk> the <unk> . <unk> </s> \n",
      "\n",
      "\n",
      "Actual: If WordPress were a country <unk> , our Bill of Rights would be the GPL because it protects our core freedoms <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The you are not bit <unk> , you <unk> is the <unk> not found best of of is the customers <unk> <unk> . <unk> </s> \n",
      "\n",
      "============= Step  2500  =============\n",
      "\t Loss:  1.82614644194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  2750\n",
      "Actual: Location good <unk> , but you need a car to get to Varna ##AT##-##AT## beaches or into city center ( or walk for <unk> through a seaside park ) <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The <unk> and , the also can to place ##AT##-##AT## the a the <unk> <unk> <unk> the the <unk> <unk> <unk> the ) the <unk> the <unk> street <unk> <unk> . <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Public parking is possible on site and costs EUR 20 <unk> per day <unk> . <unk> </s> \n",
      "\n",
      "Predicted: If parking is possible at site ( costs EUR 6 <unk> per day <unk> . <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  3000\n",
      "Actual: Ruby can load extension libraries dynamically if an OS allows <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> is be the for <unk> <unk> you email is to . <unk> </s> \n",
      "\n",
      "\n",
      "Actual: New to Casino Tropez Mobile is the Refer A Friend offer <unk> . <unk> </s> \n",
      "\n",
      "Predicted: The York the Tropez <unk> <unk> the total to <unk> <unk> <unk> . <unk> </s> \n",
      "\n",
      "============= Step  3000  =============\n",
      "\t Loss:  1.77773684335\n",
      "...................................................."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists('logs'):\n",
    "    os.mkdir('logs')\n",
    "log_dir = 'logs'\n",
    "\n",
    "bleu_scores_over_time = []\n",
    "loss_over_time = []\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "src_word_embeddings = np.load('de-embeddings.npy')\n",
    "tgt_word_embeddings = np.load('en-embeddings.npy')\n",
    "\n",
    "# Defining data generators\n",
    "enc_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=source_sequence_length,is_source=True)\n",
    "dec_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=target_sequence_length,is_source=False)\n",
    "\n",
    "num_steps = 10001\n",
    "avg_loss = 0\n",
    "\n",
    "bleu_labels, bleu_preds = [],[]\n",
    "\n",
    "print('Started Training')\n",
    "\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # input_sizes for each bin: [40]\n",
    "    # output_sizes for each bin: [60]\n",
    "    print('.',end='')\n",
    "    if (step+1)%100==0:\n",
    "        print('')\n",
    "        \n",
    "    sent_ids = np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
    "    # ====================== ENCODER DATA COLLECTION ================================================\n",
    "    \n",
    "    eu_data, eu_labels, _, eu_lengths = enc_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "    \n",
    "    feed_dict = {}\n",
    "    feed_dict[enc_train_inp_lengths] = eu_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(eu_data,eu_labels)):            \n",
    "        feed_dict[enc_train_inputs[ui]] = dat                \n",
    "    \n",
    "    # ====================== DECODER DATA COLLECITON ===========================\n",
    "    # First step we change the ids in a batch\n",
    "    du_data, du_labels, _, du_lengths = dec_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "    \n",
    "    feed_dict[dec_train_inp_lengths] = du_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(du_data,du_labels)):            \n",
    "        feed_dict[dec_train_inputs[ui]] = dat\n",
    "        feed_dict[dec_train_labels[ui]] = lbl\n",
    "        feed_dict[dec_label_masks[ui]] = (np.array([ui for _ in range(batch_size)])<du_lengths).astype(np.int32)\n",
    "    \n",
    "    # ======================= OPTIMIZATION ==========================\n",
    "    _,l,tr_pred = sess.run([sgd_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "    tr_pred = tr_pred.flatten()\n",
    "        \n",
    "            \n",
    "    if (step+1)%250==0:  \n",
    "        \n",
    "        print('Step ',step+1)\n",
    "\n",
    "        print_str = 'Actual: '\n",
    "        for w in np.concatenate(du_labels,axis=0)[::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '                    \n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "                      \n",
    "        print(print_str)\n",
    "        print()\n",
    "        \n",
    "        print_str = 'Predicted: '\n",
    "        for w in tr_pred[::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "       \n",
    "        print('\\n')  \n",
    "        \n",
    "        rand_idx = np.random.randint(low=1,high=batch_size)\n",
    "        print_str = 'Actual: '\n",
    "        for w in np.concatenate(du_labels,axis=0)[rand_idx::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "\n",
    "            \n",
    "        print()\n",
    "        print_str = 'Predicted: '\n",
    "        for w in tr_pred[rand_idx::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "        print()        \n",
    "        \n",
    "    avg_loss += l\n",
    "    \n",
    "    #sess.run(reset_train_state) # resetting hidden state for each batch\n",
    "    \n",
    "    if (step+1)%500==0:\n",
    "        print('============= Step ', str(step+1), ' =============')\n",
    "        print('\\t Loss: ',avg_loss/500.0)\n",
    "        \n",
    "        loss_over_time.append(avg_loss/500.0)\n",
    "             \n",
    "        avg_loss = 0.0\n",
    "        sess.run(inc_gstep)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
