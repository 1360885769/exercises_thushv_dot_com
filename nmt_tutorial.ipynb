{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NMT with Tensorflow seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Seq2Seq Items\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining some hyperparameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size= 50000\n",
    "num_units = 128\n",
    "input_size = 128\n",
    "batch_size = 16\n",
    "source_sequence_length=40\n",
    "target_sequence_length=60\n",
    "decoder_type = 'basic' # could be basic or attention\n",
    "sentences_to_read = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "\t [('ausgestattetes', 16818), ('Firmengeschichte', 40229), ('fürchterlichen', 39481), ('Melancholie', 49659), ('Route', 5970), ('Lobbyisten', 11946), ('zuweilen', 9260), ('Computer-', 32843), ('Produktreihe', 27955), ('Nervensystem', 32983)]\n",
      "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, ','), (4, '.'), (5, 'die'), (6, 'der'), (7, 'und'), (8, 'in'), (9, 'zu')]\n",
      "\t Vocabulary size:  50000\n",
      "Target\n",
      "\t [('vigour', 16936), ('Markets', 11144), ('screw', 10318), ('Route', 9932), ('Sates', 45188), ('roar', 37596), ('biographical', 29851), ('proportioned', 35062), ('Gozo', 26604), ('base', 1257)]\n",
      "\t [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, 'the'), (4, ','), (5, '.'), (6, 'of'), (7, 'and'), (8, 'to'), (9, 'in')]\n",
      "\t Vocabulary size:  50000\n"
     ]
    }
   ],
   "source": [
    "src_dictionary = dict()\n",
    "with open('vocab.50K.de.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        #we are discarding last char as it is new line char\n",
    "        src_dictionary[line[:-1]] = len(src_dictionary)\n",
    "\n",
    "src_reverse_dictionary = dict(zip(src_dictionary.values(),src_dictionary.keys()))\n",
    "\n",
    "print('Source')\n",
    "print('\\t',list(src_dictionary.items())[:10])\n",
    "print('\\t',list(src_reverse_dictionary.items())[:10])\n",
    "print('\\t','Vocabulary size: ', len(src_dictionary))\n",
    "\n",
    "tgt_dictionary = dict()\n",
    "with open('vocab.50K.en.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        #we are discarding last char as it is new line char\n",
    "        tgt_dictionary[line[:-1]] = len(tgt_dictionary)\n",
    "\n",
    "tgt_reverse_dictionary = dict(zip(tgt_dictionary.values(),tgt_dictionary.keys()))\n",
    "\n",
    "print('Target')\n",
    "print('\\t',list(tgt_dictionary.items())[:10])\n",
    "print('\\t',list(tgt_reverse_dictionary.items())[:10])\n",
    "print('\\t','Vocabulary size: ', len(tgt_dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Sentences (English and German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample translations (50000)\n",
      "( 0 ) DE:  Heute verstehen sich QuarkXPress ® 8 , Photoshop ® und Illustrator ® besser als jemals zuvor . Dank HTML und CSS ­ können Anwender von QuarkXPress inzwischen alle Medien bedienen , und das unabhängig von Anwendungen der Adobe ® Creative Suite ® wie Adobe Flash ® ( SWF ) und Adobe Dreamweaver ® .\n",
      "\n",
      "( 0 ) EN:  Today , QuarkXPress ® 8 has tighter integration with Photoshop ® and Illustrator ® than ever before , and through standards like HTML and CSS , QuarkXPress users can publish across media both independently and alongside Adobe ® Creative Suite ® applications like Adobe Flash ® ( SWF ) and Adobe Dreamweaver ® .\n",
      "\n",
      "( 10000 ) DE:  Es existieren Busverbindungen in nahezu jeden Ort der Provence ( eventuell mit Umsteigen in Aix ##AT##-##AT## en ##AT##-##AT## Provence ) , allerdings sollte beachtet werden , dass die letzten Busse abends ca. um 19 Uhr fahren .\n",
      "\n",
      "( 10000 ) EN:  As always in France those highways are expensive but practical , comfortable and fast .\n",
      "\n",
      "( 20000 ) DE:  Es war staubig , das Bad schmutzig . Sogar die Beleuchtung an der Wand im Flur ( Seitengebäude ) war richtig verstaubt .\n",
      "\n",
      "( 20000 ) EN:  It was rather old fashioned in the decoration .\n",
      "\n",
      "( 30000 ) DE:  Auch ist , so denkt Dr. Gutherz , bereits die erste Seite sehr viel versprechend , da sie eine Definition des klinischen Psychotrauma ##AT##-##AT## Begriffes enthält , der er gänzlich zustimmen kann .\n",
      "\n",
      "( 30000 ) EN:  At the rhetorical climax of this summary , Dr Goodheart comes across some sentences expressed with great pathos .\n",
      "\n",
      "( 40000 ) DE:  Bei einer digitalen Bildkette wird das Intensitätssignal für jedes Pixel ohne analoge Zwischenschritte direkt in der Detektoreinheit digitalisiert , d.h. in Zahlen umgewandelt .\n",
      "\n",
      "( 40000 ) EN:  A digital image chain is an image chain that is equipped with a digital detector instead of an analogue one .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_sent = []\n",
    "target_sent = []\n",
    "\n",
    "test_source_sent = []\n",
    "test_target_sent = []\n",
    "\n",
    "\n",
    "with open('train.de', encoding='utf-8') as f:\n",
    "    for l_i, line in enumerate(f):\n",
    "        # discarding first 20 translations as there was some\n",
    "        # english to english translations found in the first few. which are wrong\n",
    "        if l_i<50:\n",
    "            continue\n",
    "        source_sent.append(line)\n",
    "        if len(source_sent)>=sentences_to_read:\n",
    "            break\n",
    "        \n",
    "            \n",
    "with open('train.en', encoding='utf-8') as f:\n",
    "    for l_i, line in enumerate(f):\n",
    "        if l_i<50:\n",
    "            continue\n",
    "        \n",
    "        target_sent.append(line)\n",
    "        if len(target_sent)>=sentences_to_read:\n",
    "            break\n",
    "        \n",
    "            \n",
    "assert len(source_sent)==len(target_sent),'Source: %d, Target: %d'%(len(source_sent),len(target_sent))\n",
    "\n",
    "print('Sample translations (%d)'%len(source_sent))\n",
    "for i in range(0,sentences_to_read,10000):\n",
    "    print('(',i,') DE: ', source_sent[i])\n",
    "    print('(',i,') EN: ', target_sent[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's analyse some statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Source) Sentence mean length:  26.35934\n",
      "(Source) Sentence stddev length:  13.9681614669\n",
      "(Target) Sentence mean length:  28.58758\n",
      "(Target) Sentence stddev length:  15.1544201388\n"
     ]
    }
   ],
   "source": [
    "def split_to_tokens(sent,is_source):\n",
    "    #sent = sent.replace('-',' ')\n",
    "    sent = sent.replace(',',' ,')\n",
    "    sent = sent.replace('.',' .')\n",
    "    sent = sent.replace('\\n',' ') \n",
    "    \n",
    "    sent_toks = sent.split(' ')\n",
    "    for t_i, tok in enumerate(sent_toks):\n",
    "        if is_source:\n",
    "            if tok not in src_dictionary.keys():\n",
    "                sent_toks[t_i] = '<unk>'\n",
    "        else:\n",
    "            if tok not in tgt_dictionary.keys():\n",
    "                sent_toks[t_i] = '<unk>'\n",
    "    return sent_toks\n",
    "\n",
    "# Let us first look at some statistics of the sentences\n",
    "source_len = []\n",
    "source_mean, source_std = 0,0\n",
    "for sent in source_sent:\n",
    "    source_len.append(len(split_to_tokens(sent,True)))\n",
    "\n",
    "print('(Source) Sentence mean length: ', np.mean(source_len))\n",
    "print('(Source) Sentence stddev length: ', np.std(source_len))\n",
    "\n",
    "target_len = []\n",
    "target_mean, target_std = 0,0\n",
    "for sent in target_sent:\n",
    "    target_len.append(len(split_to_tokens(sent,False)))\n",
    "\n",
    "print('(Target) Sentence mean length: ', np.mean(target_len))\n",
    "print('(Target) Sentence stddev length: ', np.std(target_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the special tokens and make all sentences same length (for batch-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sent lengths:  0\n",
      "Samples from bin\n",
      "\t ['<s>', 'Heute', 'verstehen', 'sich', 'QuarkXPress', '®', '8', '<unk>', ',', 'Photoshop', '®', 'und', 'Illustrator', '®', 'besser', 'als', 'jemals', 'zuvor', '<unk>', '.', 'Dank', 'HTML', 'und', 'CSS', '\\xad', 'können', 'Anwender', 'von', 'QuarkXPress', 'inzwischen', 'alle', 'Medien', 'bedienen', '<unk>', ',', 'und', 'das', 'unabhängig', 'von', 'Anwendungen', 'der']\n",
      "\t ['</s>', 'Today', '<unk>', ',', 'QuarkXPress', '®', '8', 'has', 'tighter', 'integration', 'with', 'Photoshop', '®', 'and', 'Illustrator', '®', 'than', 'ever', 'before', '<unk>', ',', 'and', 'through', 'standards', 'like', 'HTML', 'and', 'CSS', '<unk>', ',', 'QuarkXPress', 'users', 'can', 'publish', 'across', 'media', 'both', 'independently', 'and', 'alongside', 'Adobe', '®', 'Creative', 'Suite', '®', 'applications', 'like', 'Adobe', 'Flash', '®', '(', 'SWF', ')', 'and', 'Adobe', 'Dreamweaver', '®', '<unk>', '.', '<unk>', '</s>']\n",
      "\t ['<s>', 'Erstellen', 'Sie', 'einen', 'Rahmen', 'und', 'gehen', 'Sie', 'dann', 'auf', 'Datei', '&gt;', 'Importieren', '<unk>', '.', '.', '.', 'oder', 'ziehen', 'Sie', 'das', 'Bild', 'einfach', 'per', 'Drag', '&amp;', 'Drop', 'von', 'Ihrem', 'Desktop', '<unk>', ',', 'aus', 'dem', 'Finder', 'oder', 'einer', 'Anwendung', 'wie', 'Adobe', 'Bridge']\n",
      "\t ['</s>', 'Bringing', 'the', 'PSD', 'files', 'into', 'QuarkXPress', 'is', 'the', 'same', 'as', 'any', 'other', 'image', '<unk>', '.', 'Create', 'a', 'Box', 'and', 'then', 'use', 'File', '&gt;', 'Import', '<unk>', '.', '.', '.', 'or', 'simply', 'drag', 'and', 'drop', 'the', 'image', 'from', 'your', 'desktop', '<unk>', ',', 'Finder', 'or', 'an', 'application', 'like', 'Adobe', 'Bridge', '®', 'with', 'or', 'without', 'creating', 'a', 'box', 'first', '<unk>', '.', '<unk>', '</s>', '</s>']\n",
      "\n",
      "\tSentences  50000\n"
     ]
    }
   ],
   "source": [
    "train_inputs = []\n",
    "train_outputs = []\n",
    "train_inp_lengths = []\n",
    "train_out_lengths = []\n",
    "\n",
    "max_tgt_sent_lengths = 0\n",
    "\n",
    "src_max_sent_length = 41\n",
    "tgt_max_sent_length = 61\n",
    "for s_i, (src_sent, tgt_sent) in enumerate(zip(source_sent,target_sent)):\n",
    "    \n",
    "    src_sent_tokens = split_to_tokens(src_sent,True)\n",
    "    tgt_sent_tokens = split_to_tokens(tgt_sent,False)\n",
    "        \n",
    "    num_src_sent = []\n",
    "    for tok in src_sent_tokens:\n",
    "        num_src_sent.append(src_dictionary[tok])\n",
    "\n",
    "    num_src_set = num_src_sent[::-1] # we reverse the source sentence. This improves performance\n",
    "    num_src_sent.insert(0,src_dictionary['<s>'])\n",
    "    train_inp_lengths.append(min(len(num_src_sent)+1,src_max_sent_length))\n",
    "    \n",
    "    # append until the sentence reaches max length\n",
    "    if len(num_src_sent)<src_max_sent_length:\n",
    "        num_src_sent.extend([src_dictionary['</s>'] for _ in range(src_max_sent_length - len(num_src_sent))])\n",
    "    # if more than max length, truncate the sentence\n",
    "    elif len(num_src_sent)>src_max_sent_length:\n",
    "        num_src_sent = num_src_sent[:src_max_sent_length]\n",
    "    assert len(num_src_sent)==src_max_sent_length,len(num_src_sent)\n",
    "\n",
    "    train_inputs.append(num_src_sent)\n",
    "\n",
    "    num_tgt_sent = [tgt_dictionary['</s>']]\n",
    "    for tok in tgt_sent_tokens:\n",
    "        num_tgt_sent.append(tgt_dictionary[tok])\n",
    "    \n",
    "    train_out_lengths.append(min(len(num_tgt_sent)+1,tgt_max_sent_length))\n",
    "    \n",
    "    if len(num_tgt_sent)<tgt_max_sent_length:\n",
    "        num_tgt_sent.extend([tgt_dictionary['</s>'] for _ in range(tgt_max_sent_length - len(num_tgt_sent))])\n",
    "    elif len(num_tgt_sent)>tgt_max_sent_length:\n",
    "        num_tgt_sent = num_tgt_sent[:tgt_max_sent_length]\n",
    "    \n",
    "    train_outputs.append(num_tgt_sent)\n",
    "    assert len(train_outputs[s_i])==tgt_max_sent_length, 'Sent length needs to be 60, but is %d'%len(binned_outputs[s_i])    \n",
    "\n",
    "assert len(train_inputs)  == len(source_sent),\\\n",
    "        'Size of total bin elements: %d, Total sentences: %d'\\\n",
    "                %(len(train_inputs),len(source_sent))\n",
    "\n",
    "print('Max sent lengths: ', max_tgt_sent_lengths)\n",
    "\n",
    "\n",
    "train_inputs = np.array(train_inputs, dtype=np.int32)\n",
    "train_outputs = np.array(train_outputs, dtype=np.int32)\n",
    "train_inp_lengths = np.array(train_inp_lengths, dtype=np.int32)\n",
    "train_out_lengths = np.array(train_out_lengths, dtype=np.int32)\n",
    "print('Samples from bin')\n",
    "print('\\t',[src_reverse_dictionary[w]  for w in train_inputs[0,:].tolist()])\n",
    "print('\\t',[tgt_reverse_dictionary[w]  for w in train_outputs[0,:].tolist()])\n",
    "print('\\t',[src_reverse_dictionary[w]  for w in train_inputs[10,:].tolist()])\n",
    "print('\\t',[tgt_reverse_dictionary[w]  for w in train_outputs[10,:].tolist()])\n",
    "print()\n",
    "print('\\tSentences ',train_inputs.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source data\n",
      "['Heute', 'Hier', 'Sie', 'Häufig', 'In']\n",
      "['verstehen', 'erfahren', 'werden', 'wird', 'diesem']\n",
      "['sich', 'Sie', 'überrascht', 'die', 'Abschnitt']\n",
      "['QuarkXPress', '<unk>', 'sein', 'Meinung', 'erläutern']\n",
      "['®', ',', '<unk>', 'vertreten', 'wir']\n",
      "['8', 'wie', ',', '<unk>', '<unk>']\n",
      "['<unk>', 'Sie', 'wie', ',', ',']\n",
      "[',', 'Creative', 'einfach', 'dass', 'wann']\n",
      "['Photoshop', 'Suite', 'sich', 'QuarkXPress', 'Sie']\n",
      "['®', '2', 'mit', '8', 'für']\n",
      "['und', 'und', 'Quark', 'von', 'Ihre']\n",
      "['Illustrator', 'Creative', 'das', 'allen', 'Bilder']\n",
      "['®', 'Suite', 'volle', 'heute', 'das']\n",
      "['besser', '3', 'Potenzial', 'verfügbaren', 'PSD']\n",
      "['als', 'am', 'Ihrer', 'Layout', '##AT##-##AT##']\n",
      "['jemals', 'besten', 'Design', '##AT##-##AT##', 'Format']\n",
      "['zuvor', 'zusammen', '##AT##-##AT##', 'Programmen', 'verwenden']\n",
      "['<unk>', 'mit', 'Software', 'die', 'sollten']\n",
      "['.', 'QuarkXPress', 'erschließen', 'beste', 'und']\n",
      "['Dank', 'nutzen', 'lässt', 'Integration', 'wie']\n",
      "['HTML', 'können', '<unk>', 'mit', 'Sie']\n",
      "['und', '<unk>', '.', 'Photoshop', 'es']\n",
      "['CSS', '.', '<unk>', 'über', 'für']\n",
      "['\\xad', '<unk>', '</s>', 'das', 'Ihre']\n",
      "['können', '</s>', '</s>', 'PSD', 'Bilder']\n",
      "['Anwender', '</s>', '</s>', '##AT##-##AT##', 'optimal']\n",
      "['von', '</s>', '</s>', 'Dateiformat', 'nutzen']\n",
      "['QuarkXPress', '</s>', '</s>', 'bietet', '<unk>']\n",
      "['inzwischen', '</s>', '</s>', '<unk>', '.']\n",
      "['alle', '</s>', '</s>', '.', '<unk>']\n",
      "['Medien', '</s>', '</s>', '<unk>', '</s>']\n",
      "['bedienen', '</s>', '</s>', '</s>', '</s>']\n",
      "['<unk>', '</s>', '</s>', '</s>', '</s>']\n",
      "[',', '</s>', '</s>', '</s>', '</s>']\n",
      "['und', '</s>', '</s>', '</s>', '</s>']\n",
      "['das', '</s>', '</s>', '</s>', '</s>']\n",
      "['unabhängig', '</s>', '</s>', '</s>', '</s>']\n",
      "['von', '</s>', '</s>', '</s>', '</s>']\n",
      "['Anwendungen', '</s>', '</s>', '</s>', '</s>']\n",
      "['der', '</s>', '</s>', '</s>', '</s>']\n",
      "\n",
      "Target data batch (first time)\n",
      "['Today', 'You', 'QuarkXPress', 'In', 'For']\n",
      "['<unk>', '’', '8', 'this', 'example']\n",
      "[',', 'll', 'is', 'section', '<unk>']\n",
      "['QuarkXPress', 'be', 'considered', 'we', ',']\n",
      "['®', 'surprised', 'by', '’', 'you']\n",
      "['8', 'how', 'many', 'll', 'may']\n",
      "['has', 'easy', 'to', 'explain', 'have']\n",
      "['tighter', 'Quark', 'have', 'when', 'multiple']\n",
      "['integration', 'has', 'the', 'you', 'layers']\n",
      "['with', 'made', 'best', 'should', 'in']\n",
      "['Photoshop', 'it', 'integration', 'use', 'your']\n",
      "['®', 'to', 'with', 'the', 'PSD']\n",
      "['and', 'unlock', 'Photoshop', 'PSD', 'with']\n",
      "['Illustrator', 'the', '’', 'format', 'different']\n",
      "['®', 'full', 's', 'for', 'product']\n",
      "['than', 'potential', 'PSD', 'your', 'shots']\n",
      "['ever', 'of', 'file', 'images', '<unk>']\n",
      "['before', 'all', 'format', 'and', ',']\n",
      "['<unk>', 'your', 'of', 'how', 'which']\n",
      "[',', 'design', 'any', 'to', 'will']\n",
      "['and', 'software', 'layout', 'get', 'vary']\n",
      "['through', '<unk>', 'tool', 'the', 'from']\n",
      "['standards', '.', 'available', 'most', 'publication']\n",
      "['like', '<unk>', 'today', 'out', 'to']\n",
      "['HTML', '</s>', '<unk>', 'of', 'publication']\n",
      "['and', '</s>', '.', 'them', '<unk>']\n",
      "['CSS', '</s>', '<unk>', '<unk>', '.']\n",
      "['<unk>', '</s>', '</s>', '.', '<unk>']\n",
      "[',', '</s>', '</s>', '<unk>', '</s>']\n",
      "['QuarkXPress', '</s>', '</s>', '</s>', '</s>']\n",
      "['users', '</s>', '</s>', '</s>', '</s>']\n",
      "['can', '</s>', '</s>', '</s>', '</s>']\n",
      "['publish', '</s>', '</s>', '</s>', '</s>']\n",
      "['across', '</s>', '</s>', '</s>', '</s>']\n",
      "['media', '</s>', '</s>', '</s>', '</s>']\n",
      "['both', '</s>', '</s>', '</s>', '</s>']\n",
      "['independently', '</s>', '</s>', '</s>', '</s>']\n",
      "['and', '</s>', '</s>', '</s>', '</s>']\n",
      "['alongside', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['Creative', '</s>', '</s>', '</s>', '</s>']\n",
      "['Suite', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['applications', '</s>', '</s>', '</s>', '</s>']\n",
      "['like', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['Flash', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['(', '</s>', '</s>', '</s>', '</s>']\n",
      "['SWF', '</s>', '</s>', '</s>', '</s>']\n",
      "[')', '</s>', '</s>', '</s>', '</s>']\n",
      "['and', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['Dreamweaver', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['<unk>', '</s>', '</s>', '</s>', '</s>']\n",
      "['.', '</s>', '</s>', '</s>', '</s>']\n",
      "['<unk>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "\n",
      "Target data batch (non-first time)\n",
      "['Today', 'You', 'QuarkXPress', 'In', 'For']\n",
      "['<unk>', '’', '8', 'this', 'example']\n",
      "[',', 'll', 'is', 'section', '<unk>']\n",
      "['QuarkXPress', 'be', 'considered', 'we', ',']\n",
      "['®', 'surprised', 'by', '’', 'you']\n",
      "['8', 'how', 'many', 'll', 'may']\n",
      "['has', 'easy', 'to', 'explain', 'have']\n",
      "['tighter', 'Quark', 'have', 'when', 'multiple']\n",
      "['integration', 'has', 'the', 'you', 'layers']\n",
      "['with', 'made', 'best', 'should', 'in']\n",
      "['Photoshop', 'it', 'integration', 'use', 'your']\n",
      "['®', 'to', 'with', 'the', 'PSD']\n",
      "['and', 'unlock', 'Photoshop', 'PSD', 'with']\n",
      "['Illustrator', 'the', '’', 'format', 'different']\n",
      "['®', 'full', 's', 'for', 'product']\n",
      "['than', 'potential', 'PSD', 'your', 'shots']\n",
      "['ever', 'of', 'file', 'images', '<unk>']\n",
      "['before', 'all', 'format', 'and', ',']\n",
      "['<unk>', 'your', 'of', 'how', 'which']\n",
      "[',', 'design', 'any', 'to', 'will']\n",
      "['and', 'software', 'layout', 'get', 'vary']\n",
      "['through', '<unk>', 'tool', 'the', 'from']\n",
      "['standards', '.', 'available', 'most', 'publication']\n",
      "['like', '<unk>', 'today', 'out', 'to']\n",
      "['HTML', '</s>', '<unk>', 'of', 'publication']\n",
      "['and', '</s>', '.', 'them', '<unk>']\n",
      "['CSS', '</s>', '<unk>', '<unk>', '.']\n",
      "['<unk>', '</s>', '</s>', '.', '<unk>']\n",
      "[',', '</s>', '</s>', '<unk>', '</s>']\n",
      "['QuarkXPress', '</s>', '</s>', '</s>', '</s>']\n",
      "['users', '</s>', '</s>', '</s>', '</s>']\n",
      "['can', '</s>', '</s>', '</s>', '</s>']\n",
      "['publish', '</s>', '</s>', '</s>', '</s>']\n",
      "['across', '</s>', '</s>', '</s>', '</s>']\n",
      "['media', '</s>', '</s>', '</s>', '</s>']\n",
      "['both', '</s>', '</s>', '</s>', '</s>']\n",
      "['independently', '</s>', '</s>', '</s>', '</s>']\n",
      "['and', '</s>', '</s>', '</s>', '</s>']\n",
      "['alongside', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['Creative', '</s>', '</s>', '</s>', '</s>']\n",
      "['Suite', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['applications', '</s>', '</s>', '</s>', '</s>']\n",
      "['like', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['Flash', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['(', '</s>', '</s>', '</s>', '</s>']\n",
      "['SWF', '</s>', '</s>', '</s>', '</s>']\n",
      "[')', '</s>', '</s>', '</s>', '</s>']\n",
      "['and', '</s>', '</s>', '</s>', '</s>']\n",
      "['Adobe', '</s>', '</s>', '</s>', '</s>']\n",
      "['Dreamweaver', '</s>', '</s>', '</s>', '</s>']\n",
      "['®', '</s>', '</s>', '</s>', '</s>']\n",
      "['<unk>', '</s>', '</s>', '</s>', '</s>']\n",
      "['.', '</s>', '</s>', '</s>', '</s>']\n",
      "['<unk>', '</s>', '</s>', '</s>', '</s>']\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "input_size = 128\n",
    "\n",
    "class DataGeneratorMT(object):\n",
    "    \n",
    "    def __init__(self,batch_size,num_unroll,is_source):\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        \n",
    "        \n",
    "        self._src_word_embeddings = np.load('de-embeddings.npy')\n",
    "        \n",
    "        self._tgt_word_embeddings = np.load('en-embeddings.npy')\n",
    "        \n",
    "        self._sent_ids = None\n",
    "        \n",
    "        self._is_source = is_source\n",
    "        \n",
    "                \n",
    "    def next_batch(self, sent_ids, first_set):\n",
    "        \n",
    "        if self._is_source:\n",
    "            max_sent_length = src_max_sent_length\n",
    "        else:\n",
    "            max_sent_length = tgt_max_sent_length\n",
    "        batch_labels_ind = []\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        \n",
    "        for b in range(self._batch_size):\n",
    "            \n",
    "            sent_id = sent_ids[b]\n",
    "            \n",
    "            if self._is_source:\n",
    "                sent_text = train_inputs[sent_id]\n",
    "                             \n",
    "                batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b]=sent_text[self._cursor[b]+1]\n",
    "\n",
    "            else:\n",
    "                sent_text = train_outputs[sent_id]\n",
    "                \n",
    "                # We cannot avoid having two different embedding vectors for <s> token\n",
    "                # in soruce and target languages\n",
    "                # Therefore, if the symbol appears, we always take the source embedding vector\n",
    "                if sent_text[self._cursor[b]]!=src_dictionary['<s>']:\n",
    "                    batch_data[b] = sent_text[self._cursor[b]]\n",
    "                else:\n",
    "                    batch_data[b] = sent_text[self._cursor[b]]\n",
    "                batch_labels[b] = sent_text[self._cursor[b]+1]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%(max_sent_length-1)\n",
    "                                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self,sent_ids):\n",
    "        \n",
    "        if sent_ids is not None:\n",
    "            \n",
    "            self._sent_ids = sent_ids\n",
    "            \n",
    "            #if self._is_source:\n",
    "                # we dont star at the very beginning, becaues the very beginning is a bunch of </s> symbols.\n",
    "                # so we start from the middel s.t we get a minimum number of </s> symbols in our training data\n",
    "                # this is only needed for source language\n",
    "                #self._cursor = ((start_indices_for_bins[bin_id][self._sent_ids]//self._num_unroll)*self._num_unroll).tolist()\n",
    "            #else:\n",
    "            self._cursor = [0 for _ in range(self._batch_size)]\n",
    "                \n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        inp_lengths = None\n",
    "        for ui in range(self._num_unroll):\n",
    "            # The first batch in any batch of captions is different\n",
    "            if self._is_source:\n",
    "                data, labels = self.next_batch(self._sent_ids, False)\n",
    "            else:\n",
    "                data, labels = self.next_batch(self._sent_ids, False)\n",
    "                    \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "            inp_lengths = train_inp_lengths[sent_ids]\n",
    "        return unroll_data, unroll_labels, self._sent_ids, inp_lengths\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        self._cursor = [0 for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if the implementation correct\n",
    "dg = DataGeneratorMT(batch_size=5,num_unroll=40,is_source=True)\n",
    "u_data, u_labels, _, _ = dg.unroll_batches([0,1,2,3,4])\n",
    "\n",
    "print('Source data')\n",
    "for _, lbl in zip(u_data,u_labels):\n",
    "    print([src_reverse_dictionary[w] for w in lbl.tolist()])\n",
    "\n",
    "        \n",
    "# Running a tiny set to see if the implementation correct\n",
    "dg = DataGeneratorMT(batch_size=5,num_unroll=60,is_source=False)\n",
    "u_data, u_labels, _, _ = dg.unroll_batches([0,2,3,4,5])\n",
    "print('\\nTarget data batch (first time)')\n",
    "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
    "    #if d_i>5 and d_i < 35:\n",
    "    #    continue\n",
    "\n",
    "    print([tgt_reverse_dictionary[w] for w in lbl.tolist()])\n",
    "\n",
    "print('\\nTarget data batch (non-first time)')\n",
    "u_data, u_labels, _, _ = dg.unroll_batches(None)\n",
    "for d_i,(_, lbl) in enumerate(zip(u_data,u_labels)):\n",
    "    \n",
    "    #if d_i>5 and d_i < 35:\n",
    "    #    continue\n",
    "        \n",
    "    print([tgt_reverse_dictionary[w] for w in lbl.tolist()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs Outputs Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "enc_train_inputs = []\n",
    "dec_train_inputs = []\n",
    "\n",
    "# Need to use pre-trained word embeddings\n",
    "encoder_emb_layer = tf.convert_to_tensor(np.load('de-embeddings.npy'))\n",
    "decoder_emb_layer = tf.convert_to_tensor(np.load('en-embeddings.npy'))\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(source_sequence_length):\n",
    "    enc_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size],name='enc_train_inputs_%d'%ui))\n",
    "\n",
    "dec_train_labels=[]\n",
    "dec_label_masks = []\n",
    "for ui in range(target_sequence_length):\n",
    "    dec_train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size],name='dec_train_inputs_%d'%ui))\n",
    "    dec_train_labels.append(tf.placeholder(tf.int32, shape=[batch_size],name='dec-train_outputs_%d'%ui))\n",
    "    dec_label_masks.append(tf.placeholder(tf.float32, shape=[batch_size],name='dec-label_masks_%d'%ui))\n",
    "    \n",
    "encoder_emb_inp = [tf.nn.embedding_lookup(encoder_emb_layer, src) for src in enc_train_inputs]\n",
    "encoder_emb_inp = tf.stack(encoder_emb_inp)\n",
    "\n",
    "decoder_emb_inp = [tf.nn.embedding_lookup(decoder_emb_layer, src) for src in dec_train_inputs]\n",
    "decoder_emb_inp = tf.stack(decoder_emb_inp)\n",
    "\n",
    "enc_train_inp_lengths = tf.placeholder(tf.int32, shape=[batch_size],name='train_input_lengths')\n",
    "dec_train_inp_lengths = tf.placeholder(tf.int32, shape=[batch_size],name='train_output_lengths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_emb_inp, initial_state=initial_state,\n",
    "    sequence_length=enc_train_inp_lengths, \n",
    "    time_major=True, swap_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build RNN cell\n",
    "decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "\n",
    "projection_layer = Dense(units=vocab_size, use_bias=True)\n",
    "\n",
    "# Helper\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "    decoder_emb_inp, [tgt_max_sent_length-1 for _ in range(batch_size)], time_major=True)\n",
    "\n",
    "# Decoder\n",
    "if decoder_type == 'basic':\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, encoder_state,\n",
    "        output_layer=projection_layer)\n",
    "    \n",
    "elif decoder_type == 'attention':\n",
    "    decoder = tf.contrib.seq2seq.BahdanauAttention(\n",
    "        decoder_cell, helper, encoder_state,\n",
    "        output_layer=projection_layer)\n",
    "    \n",
    "# Dynamic decoding\n",
    "outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder, output_time_major=True,\n",
    "    swap_memory=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = outputs.rnn_output\n",
    "\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=dec_train_labels, logits=logits)\n",
    "loss = (tf.reduce_sum(crossent*tf.stack(dec_label_masks)) / (batch_size*target_sequence_length))\n",
    "\n",
    "train_prediction = outputs.sample_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Optimizer with Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Optimizer\n"
     ]
    }
   ],
   "source": [
    "print('Defining Optimizer')\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    0.01, global_step, decay_steps=10, decay_rate=0.9, staircase=True)\n",
    "\n",
    "with tf.variable_scope('Adam'):\n",
    "    adam_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "adam_gradients, v = zip(*adam_optimizer.compute_gradients(loss))\n",
    "adam_gradients, _ = tf.clip_by_global_norm(adam_gradients, 25.0)\n",
    "adam_optimize = adam_optimizer.apply_gradients(zip(adam_gradients, v))\n",
    "\n",
    "with tf.variable_scope('SGD'):\n",
    "    sgd_optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "sgd_gradients, v = zip(*sgd_optimizer.compute_gradients(loss))\n",
    "sgd_gradients, _ = tf.clip_by_global_norm(sgd_gradients, 25.0)\n",
    "sgd_optimize = sgd_optimizer.apply_gradients(zip(sgd_gradients, v))\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  250\n",
      "Actual: We should consider all possible means of preventing those who still remain <unk> from getting the virus <unk> . But we must keep in mind too the 34 million Africans currently <unk> positive or ill with full ##AT##-##AT## blown Aids <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Greece and Greek Cyprus <unk> , for example <unk> , have as Eastern Orthodox a tradition as those parts of Eastern Europe that fell under communism <unk> , but because they remained in the West European orbit <unk> , these two countries have substantially modernised and made rapid economic progress in the past half century <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  500\n",
      "Actual: We offer remote PC and laptop control of PXI <unk> , network control over Ethernet <unk> , embedded controllers with Microsoft <unk> <unk> , and real ##AT##-##AT## time embedded controllers using LabVIEW Real ##AT##-##AT## Time <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Allied to a united Europe <unk> , she would then still retain the chance of being able to play the part of arbiter in world affairs <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  500  =============\n",
      "\t Loss:  4.74630721188\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  750\n",
      "Actual: In Queens <unk> , numbers identify not only avenues and streets <unk> , but also roads <unk> , places <unk> , and lanes <unk> , all of which might be near each other <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Room Notes : Classic twin room with en suite bath and shower <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  1000\n",
      "Actual: Often matrix configurations grow beyond the size of a single module <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Hotel Description : This 5 ##AT##-##AT## storey hotel is air ##AT##-##AT## conditioned and comprises a total of 259 rooms <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  1000  =============\n",
      "\t Loss:  4.69687741375\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  1250\n",
      "Actual: The apartment is located in the handsome La Latina area of central Madrid <unk> , well ##AT##-##AT## known for the famous open ##AT##-##AT## air El <unk> market <unk> , which is just a few minutes walk away <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: This hotel delivers great value for money <unk> . We stayed for a week in a Standard room with a balcony and mountain view <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  1500\n",
      "Actual: Our team receives training on the technical specifications of your products and services and is able to manage and solve any issues your clients may have <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Some forum users have expressed a dislike of Larry <unk> . <unk> </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  1500  =============\n",
      "\t Loss:  4.66760753822\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  1750\n",
      "Actual: <unk> 2 nights <unk> . . . . . . . . . . . . . get a free voucher worth 40 Euros for two in our restaurant &apos; <unk> &apos;s Delight &apos; so you can try our organic cuisine <unk> . <unk> </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "\n",
      "Actual: And we must – with Beckett and Foucault – answer : &quot; What matter who &apos;s speaking ? <unk> </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  2000\n",
      "Actual: Ambra Palace Hotel also features two meeting rooms that <unk> up to 55 people <unk> , banquets can be organised for up to 40 people <unk> , free internet point and wi ##AT##-##AT## fi connection in hotel lobby and in the meeting rooms <unk> , and garage with <unk> service or guarded indoor parking <unk> . <unk> </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "\n",
      "Actual: Will you succeed in bringing <unk> to your side ? <unk> </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  2000  =============\n",
      "\t Loss:  4.61245883799\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  2250\n",
      "Actual: If you don &apos;t have this access <unk> , simply contact your service provider <unk> , who will be able to connect you and provide you with answers to any queries you may have regarding adding the internet to your phone package <unk> . <unk> </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "\n",
      "Actual: Roof terrace is a bonus <unk> , sitting up there on a nice day with <unk> <unk> <unk> , just lovely <unk> , plus beats getting ripped off on overpriced drinks on any other roof terrace ! ! <unk> </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  2500\n",
      "Actual: The equipment <unk> / Printing production / Books <unk> , newspaper <unk> , <unk> <unk> . . . <unk> </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "\n",
      "Actual: There are a few pizzerias <unk> , mostly wood ##AT##-##AT## fired and quite acceptable <unk> . The <unk> chocolate sold at stores is delicious <unk> . <unk> </s> \n",
      "\n",
      "Predicted: </s> \n",
      "\n",
      "============= Step  2500  =============\n",
      "\t Loss:  4.33138716078\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  2750\n",
      "Actual: Why not treat yourself to one of our luxurious Beauty treatments <unk> . Including <unk> <unk> , Make Up <unk> , Tan <unk> , Hands &amp; <unk> in our <unk> Suite <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: The return value will vary based on the implementation <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  3000\n",
      "Actual: 33 O ye wicked and perverse and a <unk> people <unk> , why have ye built up churches unto yourselves to get b gain ? <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: No direct hotel hopper transfers to terminal 4 <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  3000  =============\n",
      "\t Loss:  3.75957730865\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  3250\n",
      "Actual: Maybe they only want to see boss fights <unk> , or maybe they have dial ##AT##-##AT## up and don &apos;t want to wait a week before they can watch anything <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: When would you like to stay at the Apart Hotel ##AT##-##AT## <unk> ? <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  3500\n",
      "Actual: We see the web pages your clients see <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Few people have asked themselves why this particular type of demonstrations so often <unk> into violence and destruction <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  3500  =============\n",
      "\t Loss:  3.57360361481\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  3750\n",
      "Actual: There are 13 bedrooms all with private shower or bath and toilet TV and tea maker <unk> . Some rooms have sea views at £ 6 per night supplement <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Our à la carte restaurant is open Tuesday to Thursday from 11 : 30 to 14 : 00 and from 18 : 00 to 21 : 00 <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  4000\n",
      "Actual: The only problem there might be is the <unk> heat during the summer but a typical bus ride is only around 20 minutes so it should be fine <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: He put the posters on the wall <unk> . ( Note : The <unk> a alternatively could have been used in the second sentence <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  4000  =============\n",
      "\t Loss:  3.47159235334\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  4250\n",
      "Actual: You will find the buffet restaurant &quot; <unk> &quot; <unk> , offering international cuisine and the à la carte bar ##AT##-##AT## grill restaurant <unk> , at the pool <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: In either case <unk> , though <unk> , don &apos;t worry too much about it ; if you don &apos;t know the details of different English rules <unk> , just use what you know <unk> , and someone will fix it later <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  4500\n",
      "Actual: We thank our web partners <unk> Technical Solutions and <unk> <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
      "\n",
      "\n",
      "Actual: This accommodation in <unk> <unk> , Granada is located on the beach <unk> , in a brand new apartment complex <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
      "\n",
      "============= Step  4500  =============\n",
      "\t Loss:  3.3616649065\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  4750\n",
      "Actual: Private parking is possible on site ( reservation is needed ) and costs AUD 20 <unk> per day <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: <unk> of <unk> , displaying of <unk> , or linking to this Commons <unk> does not create an attorney ##AT##-##AT## client relationship <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  5000\n",
      "Actual: In Fuerte Hoteles <unk> , a family business founded in 1957 <unk> , we ’ re one big family and make sure this closeness extends to our guests <unk> , without losing any of the quality of service that has always been our priority <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: The guest reviews are submitted by our customers after their stay at Jackson Court Hotel <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  5000  =============\n",
      "\t Loss:  3.31632621336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  5250\n",
      "Actual: Video preview on our new fully ##AT##-##AT## automatic <unk> ##AT##-##AT## milling unit <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: The guest reviews are submitted by our customers after their stay at <unk> <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  5500\n",
      "Actual: Description : Herbert Lodge Hostel is a large Georgian Building specialising in providing serviced accommodation on a nightly <unk> , short ##AT##-##AT## term <unk> , long ##AT##-##AT## term and <unk> . . . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Accordingly <unk> , if we treat the program as though it were a familiar part of the prior art -- as well ##AT##-##AT## established precedent requires 41 -- it is absolutely clear that their application contains no claim of patentable invention <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  5500  =============\n",
      "\t Loss:  3.26538973379\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  5750\n",
      "Actual: We look forward to your shared interest in starting a new career with Siltronic <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Relax in our Leisure Club complete with swimming pool <unk> , gym and spa bath <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  6000\n",
      "Actual: Descriptions and declarations for end products containing <unk> ™ ( isomaltulose ) of course depend on the overall product design and each country ’ s specific regulatory requirements <unk> , but labels such as &quot; With long ##AT##-##AT## lasting energy formula &quot; <unk> , &quot; low glycemic &quot; or even &quot; The better energy &quot; are possible <unk> . <unk> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
      "\n",
      "\n",
      "Actual: 48 And again <unk> , verily I say unto you <unk> , my servant Joseph <unk> , that whatsoever you give on earth <unk> , and to <unk> you a give any one on earth <unk> , by my word and according to my law <unk> , it shall be visited with blessings and not <unk> <unk> , and with \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  6000  =============\n",
      "\t Loss:  3.23682436228\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  6250\n",
      "Actual: ATMs - Mastercard / Maestro withdrawals are available at Banque <unk> in <unk> <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: The guest reviews are submitted by our customers after their stay at Dover Beach Hotel <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  6500\n",
      "Actual: Conil also provides a well equipped fitness studio for all those of you <unk> , who want to work out in their in their holidays <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
      "\n",
      "\n",
      "Actual: Better yet <unk> , unlike the Rail Pass <unk> , the days do not have to be consecutive <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
      "\n",
      "============= Step  6500  =============\n",
      "\t Loss:  3.20223960114\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  6750\n",
      "Actual: I have not forgotten the one instance of apparent success <unk> , but a success that is purely material <unk> , and it is of that monster which calls itself the United States that I wish to talk <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Based on these research results I have derived devices for creating <unk> powerful pictures for emotional advertising <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  7000\n",
      "Actual: The Shaftesbury Premier London Notting Hill offers stylish <unk> , modern 4 ##AT##-##AT## star guests accommodation <unk> , well ##AT##-##AT## equipped with all the modern comforts and facilities you need in a restful <unk> , relaxing base <unk> , including air conditioning <unk> , flat ##AT##-##AT## screen TVs and complimentary broadband internet access <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: TEC ##AT##-##AT## IT releases the new TBarCode DLL Version 6 <unk> , which can be downloaded from now <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  7000  =============\n",
      "\t Loss:  3.18405698633\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  7250\n",
      "Actual: Pick ##AT##-##AT## up from the airport is free <unk> , return is at a fee <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: The centre is equipped with all the latest technology <unk> , plus interpreting booths and a control room <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  7500\n",
      "Actual: Then I have a pair of Pentax Super Program for better image quality and I &apos;m currently experimenting with two Canon EOS 1 and <unk> <unk> <unk> . The best <unk> come with wide angles <unk> <unk> . . <unk> <unk> , and I also get stunning effects with 200 <unk> . . 300mm telephoto lenses on occasions <unk> . \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: According to the special report <unk> , security operators at the airport should check each bag before putting it into the plane ; however <unk> , some operators take advantage of the scanner machine to detect valuable objects and steal them <unk> . The report states that this event occurs every day and that the stolen items include anything from \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  7500  =============\n",
      "\t Loss:  3.12375931072\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  7750\n",
      "Actual: <unk> : Information on this website regarding <unk> yacht charter catamaran is used with permission of <unk> <unk> OF <unk> <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Palatinose ™ ( isomaltulose ) is also used within sports gels such as Atlantic <unk> as well as in cereal bars such as the bars from Netherlands ##AT##-##AT## based Luke <unk> , and Body Check by <unk> <unk> , which are available in Scandinavia <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  8000\n",
      "Actual: We had a decent size room with extra bed for 14 year old grandson <unk> , flat screen TV <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: You can access the reservation form for each property by clicking on its &apos; book now &apos; button <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  8000  =============\n",
      "\t Loss:  3.13674625587\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  8250\n",
      "Actual: Brandenburg is known for its nature <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: The tobacco factory in which <unk> and <unk> have set up their practice is one of the empty industrial complexes outside the centre of the city which are at present being converted for the country &apos;s “ creative industry ” <unk> , or to make office spaces or elegant apartments <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  8500\n",
      "Actual: In the press <unk> , these various and implicit traditions that appear in Schröder &apos;s contributions to 6 June 2004 were placed in an explicit context of meta ##AT##-##AT## memory <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Then <unk> , you should see a nice black and white GRUB <unk> where you will select the kernel to boot and press Enter <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  8500  =============\n",
      "\t Loss:  3.11060073233\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  8750\n",
      "Actual: Bedrooms have a stylish contemporary décor <unk> , each room is spacious <unk> , newly decorated with king size beds and crisp white duvets <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: What are we to understand by the little book which was a eaten by John <unk> , as mentioned in the 10th chapter of Revelation ? <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  9000\n",
      "Actual: The group &apos;s objective is not to create a nursery on the camp but to offer a wide range of different activities for the kids and adults present on the camp <unk> , so as to ensure a connection with them <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: The guest reviews are submitted by our customers after their stay at Hotel Mediterraneo <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  9000  =============\n",
      "\t Loss:  3.07781981993\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "..................................................Step  9250\n",
      "Actual: As a conclusion <unk> , in the context of this analysis and of the selected model <unk> , the growth time allows to explain very well the dry weight of an onion <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Industries and companies ( from Fortune 100 to sole proprietors ) worldwide are setting the precedence for excellence and quality in their operations with Cold Jet &apos;s cutting edge design and innovation <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "..................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "Step  9500\n",
      "Actual: imagelooop reminds customers that <unk> , at present <unk> , data <unk> and data security cannot be 100 % guaranteed on open networks such as the internet <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "\n",
      "Actual: Respect that when visiting temple ruins or other ritual places and behave as if it were a church <unk> . <unk> </s> \n",
      "\n",
      "Predicted: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s> \n",
      "\n",
      "============= Step  9500  =============\n",
      "\t Loss:  3.05375219631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "....................................................................................................\n",
      "............................................"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-70979cc98dfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# ======================= OPTIMIZATION ==========================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtr_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msgd_optimize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_prediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[0mtr_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thushan\\documents\\python_virtualenvs\\tensorflow_venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thushan\\documents\\python_virtualenvs\\tensorflow_venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thushan\\documents\\python_virtualenvs\\tensorflow_venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thushan\\documents\\python_virtualenvs\\tensorflow_venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thushan\\documents\\python_virtualenvs\\tensorflow_venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists('logs'):\n",
    "    os.mkdir('logs')\n",
    "log_dir = 'logs'\n",
    "\n",
    "bleu_scores_over_time = []\n",
    "loss_over_time = []\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "src_word_embeddings = np.load('de-embeddings.npy')\n",
    "tgt_word_embeddings = np.load('en-embeddings.npy')\n",
    "\n",
    "# Defining data generators\n",
    "enc_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=source_sequence_length,is_source=True)\n",
    "dec_data_generator = DataGeneratorMT(batch_size=batch_size,num_unroll=target_sequence_length,is_source=False)\n",
    "\n",
    "num_steps = 10001\n",
    "avg_loss = 0\n",
    "\n",
    "bleu_labels, bleu_preds = [],[]\n",
    "\n",
    "print('Started Training')\n",
    "\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # input_sizes for each bin: [40]\n",
    "    # output_sizes for each bin: [60]\n",
    "    print('.',end='')\n",
    "    if (step+1)%100==0:\n",
    "        print('')\n",
    "        \n",
    "    sent_ids = np.random.randint(low=0,high=train_inputs.shape[0],size=(batch_size))\n",
    "    # ====================== ENCODER DATA COLLECTION ================================================\n",
    "    \n",
    "    eu_data, eu_labels, _, eu_lengths = enc_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "    \n",
    "    feed_dict = {}\n",
    "    feed_dict[enc_train_inp_lengths] = eu_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(eu_data,eu_labels)):            \n",
    "        feed_dict[enc_train_inputs[ui]] = dat                \n",
    "    \n",
    "    # ====================== DECODER DATA COLLECITON ===========================\n",
    "    # First step we change the ids in a batch\n",
    "    du_data, du_labels, _, du_lengths = dec_data_generator.unroll_batches(sent_ids=sent_ids)\n",
    "    \n",
    "    feed_dict[dec_train_inp_lengths] = du_lengths\n",
    "    for ui,(dat,lbl) in enumerate(zip(du_data,du_labels)):            \n",
    "        feed_dict[dec_train_inputs[ui]] = dat\n",
    "        feed_dict[dec_train_labels[ui]] = lbl\n",
    "        feed_dict[dec_label_masks[ui]] = (np.array([ui for _ in range(batch_size)])<du_lengths).astype(np.int32)\n",
    "    \n",
    "    # ======================= OPTIMIZATION ==========================\n",
    "    if step < 10000:\n",
    "        _,l,tr_pred = sess.run([adam_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "    else:\n",
    "        _,l,tr_pred = sess.run([sgd_optimize,loss,train_prediction], feed_dict=feed_dict)\n",
    "    tr_pred = tr_pred.flatten()\n",
    "        \n",
    "            \n",
    "    if (step+1)%250==0:  \n",
    "        \n",
    "        print('Step ',step+1)\n",
    "\n",
    "        print_str = 'Actual: '\n",
    "        for w in np.concatenate(du_labels,axis=0)[::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '                    \n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "                      \n",
    "        print(print_str)\n",
    "        print()\n",
    "        \n",
    "        print_str = 'Predicted: '\n",
    "        for w in tr_pred[::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "       \n",
    "        print('\\n')  \n",
    "        \n",
    "        rand_idx = np.random.randint(low=1,high=batch_size)\n",
    "        print_str = 'Actual: '\n",
    "        for w in np.concatenate(du_labels,axis=0)[rand_idx::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "\n",
    "            \n",
    "        print()\n",
    "        print_str = 'Predicted: '\n",
    "        for w in tr_pred[rand_idx::batch_size].tolist():\n",
    "            print_str += tgt_reverse_dictionary[w] + ' '\n",
    "            if tgt_reverse_dictionary[w] == '</s>':\n",
    "                break\n",
    "        print(print_str)\n",
    "        print()        \n",
    "        \n",
    "    avg_loss += l\n",
    "    \n",
    "    #sess.run(reset_train_state) # resetting hidden state for each batch\n",
    "    \n",
    "    if (step+1)%500==0:\n",
    "        print('============= Step ', str(step+1), ' =============')\n",
    "        print('\\t Loss: ',avg_loss/500.0)\n",
    "        \n",
    "        loss_over_time.append(avg_loss/500.0)\n",
    "             \n",
    "        avg_loss = 0.0\n",
    "        sess.run(inc_gstep)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
